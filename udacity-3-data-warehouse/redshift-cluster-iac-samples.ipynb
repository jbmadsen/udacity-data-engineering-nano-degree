{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all required libraries\n",
    "import pandas as pd \n",
    "import boto3\n",
    "import botocore.exceptions\n",
    "import json\n",
    "import configparser\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read the contents of the config file\n",
    "ioc_config = configparser.ConfigParser()\n",
    "ioc_config.read_file(open('./dwh-ioc.cfg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the keys needed to create AWS services\n",
    "KEY                    = ioc_config.get('AWS','KEY')\n",
    "SECRET                 = ioc_config.get('AWS','SECRET')\n",
    "\n",
    "DWH_REGION             = ioc_config.get(\"DWH\",\"DWH_REGION\")\n",
    "DWH_CLUSTER_TYPE       = ioc_config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = ioc_config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = ioc_config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = ioc_config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = ioc_config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = ioc_config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = ioc_config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = ioc_config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = ioc_config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the parameters for creating the DWH cluster\n",
    "df = pd.DataFrame({\n",
    "        \"Param\":[\"DWH_REGION\", \"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "        \"Value\":[DWH_REGION, DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "    })\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating resources/clients for all needed infrastructure: EC2, S3, IAM, Redshift\n",
    "def create_client(name, func):\n",
    "    print(\"Creating client for\", name)\n",
    "    return func(name,\n",
    "                region_name=DWH_REGION,\n",
    "                aws_access_key_id=KEY,\n",
    "                aws_secret_access_key=SECRET)\n",
    "\n",
    "\n",
    "ec2 = create_client('ec2', boto3.resource)\n",
    "s3 = create_client('s3', boto3.resource)\n",
    "iam = create_client('iam', boto3.client)\n",
    "redshift = create_client('redshift', boto3.client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating IAM role for Redshift, allowing it to use AWS services\n",
    "print(\"Creating a new IAM Role\") \n",
    "try:\n",
    "    resp = iam.create_role(Path='/',\n",
    "                           RoleName=DWH_IAM_ROLE_NAME,\n",
    "                           Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "                           AssumeRolePolicyDocument=json.dumps({'Statement': [{'Action': 'sts:AssumeRole',\n",
    "                                                                               'Effect': 'Allow',\n",
    "                                                                               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "                                                                'Version': '2012-10-17'}\n",
    "                                                              )\n",
    "                          )\n",
    "    print(\"IAM Role created\")\n",
    "except iam.exceptions.EntityAlreadyExistsException:\n",
    "    print(\"IAM Role already created\")\n",
    "except Exception as e:\n",
    "    print(\"Error creating IAM Role:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attaching policy to role, and return the ARN role \n",
    "print(\"Attaching policy to IAM role\")\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")['ResponseMetadata']['HTTPStatusCode']\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "#print(\"ARN role:\", roleArn)\n",
    "# TODO Save to dwh.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates Redshift cluster (Warning, this costs money - make sure to use it or delete it again!)\n",
    "cluster = redshift.create_cluster(\n",
    "    #Hardware provisioned\n",
    "    ClusterType=DWH_CLUSTER_TYPE,\n",
    "    NodeType=DWH_NODE_TYPE,\n",
    "    NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "    #Identifiers & Credentials\n",
    "    DBName=DWH_DB,\n",
    "    ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "    MasterUsername=DWH_DB_USER,\n",
    "    MasterUserPassword=DWH_DB_PASSWORD,\n",
    "            \n",
    "    #Roles (for s3 access)\n",
    "    IamRoles=[roleArn]  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cluster['Cluster']['NodeType'])\n",
    "# TODO: Pretty print only needed (and public) information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query status of the cluster\n",
    "def prettyRedshiftProps(props, limited = True):\n",
    "    #pd.set_option('display.max_colwidth', -1)\n",
    "    if limited:\n",
    "        keysToShow = [\"ClusterStatus\"]\n",
    "    else:\n",
    "        keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print status, sleep if not available, try again\n",
    "while True:\n",
    "    myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "    df = prettyRedshiftProps(myClusterProps, limited=True)\n",
    "    print(df.values)\n",
    "    if myClusterProps['ClusterStatus'] == 'available':\n",
    "        break\n",
    "    time.sleep(30) # Sleep 30 seconds, and look again, untill cluster becomes available\n",
    "\n",
    "# Print full details once cluster is available\n",
    "df = prettyRedshiftProps(myClusterProps, limited=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get endpoint and ARN role for cluster\n",
    "cluster_properties = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "\n",
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "#print(\"DWH_ENDPOINT:\", DWH_ENDPOINT)\n",
    "#print(\"DWH_ROLE_ARN:\", DWH_ROLE_ARN)\n",
    "# TODO: Add these to dwg.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update cluster security group to allow access through redshift port\n",
    "vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "\n",
    "# The first Security group should be the default one\n",
    "defaultSg = list(vpc.security_groups.all())[0]\n",
    "print(\"Default Security group:\", defaultSg)\n",
    "\n",
    "# Authorize access\n",
    "try:\n",
    "    defaultSg.authorize_ingress(GroupName=defaultSg.group_name,\n",
    "                                CidrIp='0.0.0.0/0',\n",
    "                                IpProtocol='TCP',\n",
    "                                FromPort=int(DWH_PORT),\n",
    "                                ToPort=int(DWH_PORT)\n",
    "                               )\n",
    "    print(\"Access authorized\")\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    print(\"ClientError:\", e)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connection\n",
    "import psycopg2\n",
    "\n",
    "try:\n",
    "    dwh_config = configparser.ConfigParser()\n",
    "    dwh_config.read_file(open('./dwh.cfg'))\n",
    "\n",
    "    conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*dwh_config['CLUSTER'].values()))\n",
    "\n",
    "    cur = conn.cursor()\n",
    "    print('Connected to AWS Redshift cluster')\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"Error connecting to Redshift cluster:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Add credentials created to dwh.cfg file automatically, so we know they are up to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Created and connection tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[['ClusterStatus' 'deleting']]\n[['ClusterStatus' 'deleting']]\n"
    }
   ],
   "source": [
    "# Test cluster status\n",
    "while True:\n",
    "    try:\n",
    "        myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "        df = prettyRedshiftProps(myClusterProps, limited=True)\n",
    "        print(df.values)\n",
    "        time.sleep(30)\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving cluster status. Assuming it has been deleted.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Teardown of cluster to save money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete cluster (will take time)\n",
    "resp = redshift.delete_cluster(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,   \n",
    "                               SkipFinalClusterSnapshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the status - I have no idea what the status will become after deletion, so no loop here\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "df = prettyRedshiftProps(myClusterProps, limited=False)\n",
    "print(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detach and delete role, since there are no cluster to use this on\n",
    "detach_resp = iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, \n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "delete_resp = iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)\n",
    "\n",
    "# TODO: Print status of these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Everything SHOULD(?) be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}